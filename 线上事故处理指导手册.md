# 线上事故处理手册（2018年3月27日 - 初稿）

## 故障发生前

- 将“线上故障处理任务”进行细分
    - 按服务类别（基础服务 or 业务）
    - 按耦合程度
    - 确定故障排查责任人，以及责任人在排查过程中必须确定的 checkpoint（取决于问题域影响范围）
    - 定义故障排查+处理的时间窗口值（取决于实际情况，讨论合理性和必要性）
    - 确定并行操作的人员角色（依赖问题域的规模）
        - A 负责服务降级
        - B 负责服务器扩容
        - C 负责核心监控曲线的巡检
        - D 负责线上数据的收集+初步分析
- 针对常规排障手段编写《线上事故处理手册》，明确定义实施操作的约束条件（因为一般情况下很难在短时间内定位并解决故障）
    - **服务降级**（什么时候，什么情况下进行）
    - **服务紧急扩容**（哪些场景）
    - **回退版本**（是否直接作为第一优先措施）
    - 枚举出之前遇到的各种需要通过“**回滚，重启，扩容**”进行问题解决的场景，并写入《线上事故处理手册》中
- 确定核心业务指标（曲线）的基准线（明确定义什么样的情况算正常，什么样的情况算异常）
    - 可能需要对业务类型进行分类
    - 可能需要基于业务采用的语言和架构模型进行分类
- 枚举出之前遇到的各种需要通过“回滚，重启，扩容”进行问题解决的场景，并写入《线上事故处理手册》中
- 针对特定问题域制定**降级方案**或**临时方案**，并写入《线上事故处理手册》中（降级方案/临时方案一般是通过削减非必要功能来保证核心功能的正常运行）


## 故障发生时

- 确定故障的**紧急程度**和**严重程度**
    - 在例行检查时主动发现的问题（建立例行检查机制）-- 不紧急+不严重
    - 系统监控告警（建立常规系统指标异常知识库） -- 视情况而定
        - 系统的 CPU 使用率
        - Load average
        - Memory
        - I/O （网络与磁盘）
        - SWAP 使用情况
        - 线程数
        - File Description 文件描述符
        - 等等
    - 业务监控告警（核心业务指标上大屏） -- 紧急+严重
        - 接口的响应时间
        - QPS
        - 调用频次
        - 接口成功率
        - 接口波动率等
    - 关联系统故障追溯（上下游系统出现问题时） -- 不紧急+可能严重
    - 生产事件上报（来自用户的问题反馈） -- 紧急+可能严重
- 确定故障影响范围
    - 是不是真正的线上故障（可能是系统变更或业务发布导致）
    - 整个服务不可用？某些服务不可用？特定服务不可用？核心服务或组件不可用？
    - 故障系统是否对上下游产生了连带影响？
- 问题类型筛查，如果条件具备，请给出针对问题原因的推断（基于历史经验，主要用于缩小问题范围，出现偏差的问题只能通过不断完善经验库解决）
    - 故障系统是否新版本发布？
    - 依赖的基础平台与资源是否升级过？
    - 依赖的系统是否上过线？升过级？
    - 是否业务量暴涨
        - 攻击
        - 特定场景下的暴涨（秒杀、大促、流量小生）
    - 上下游服务调用异常
    - 网络是否有波动？
    - 运营是否在系统内做过运营变更？
    - 运营方是否有促销活动？
    - 服务器故障（磁盘满，内存爆）
    - 数据库故障
    - 应用自身故障
- 按照线上故障紧急处理预案进行操作
    - 回滚，重启，扩容
    - 降级/临时方案


## 故障发生后

- 线上脏数据处理
- 是否能够准确描述事故的发生前因后果（最好能基于时间线描述问题）
- 是否能够将问题重现（以便更深入的挖掘问题）
- 是否监控存在缺失
- 是否在排障过程中存在不当操作
- 输出线上事故报告（经验总结和积累）
- 是否要确定责任人（主要用于提醒，非惩罚）
- 是否能将一些流程化的工作自动化（减少人为操作，提高效率，降低错误发生率）
- 哪些环节存在弱点？哪些流程/规范/制度需要优化？这类问题是否在其他系统或者团队中也存在？
- 提出整改措施
    - 类似的问题哪些地方还有可能发生？
    - 做了哪些改进，事故就不会再发生？
    - 做了哪些改进，即使发生故障，也不会产生较大影响？



