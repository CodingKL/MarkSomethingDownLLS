# AWS 系统类案例

## 目录

- [linux 调度延迟较大](#linux-调度延迟较大)

## linux 调度延迟较大

### 0x01 提供的信息

我们在线上业务发现**位于不同机器上的相同服务响应时间不一样**，有的比较低（几十毫秒），有的比较高（几百毫秒），然后我们进一步观察到响应时间高的机器同时运行了其他的业务，于是我们为了验证是否是互相抢占资源引起的问题，我们在高峰时期把响应时间高的机器上的服务迁移到了其他机器，然后观察响应时间变化，结果符合我们的预期，响应时间降下来了。

定位具体原因：接下来我们需要进一步确定是因为什么导致这样的问题，首先查看了系统的磁盘 IO/网络 IO，发现相关的统计值并不高。然后怀疑可能是因为 cpu 竞争或者频繁调度导致 L1/2/3 cache miss 引起的。我们使用的机器型号是 c4.xlarge (4core) ，但是发现 cpu 利用率并没有达到 400% ；本以为跟 cpu 无关，不过我们还是看了一下系统的 load (4.5, 4.7, 4.3) 以及相关的 runnable task 的相关统计数据；

下面是 vmstat 的输出:

```
root@ip-172-1-47-54:~# vmstat 1
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
11  0      0 340756 1069680 880364    0    0     0    12    1    1 11  3 86  0  0
 5  0      0 337108 1069680 880464    0    0     0     0 26950 28156 64  8 28  0  1
 8  0      0 336704 1069680 880492    0    0     0     0 21908 22374 65  8 27  0  0
 3  0      0 335196 1069680 880560    0    0     0     8 32693 33370 60  9 30  0  1
 9  0      0 333128 1069680 880612    0    0     0   168 15413 19282 75  7 17  0  0
 7  0      0 333584 1069680 880616    0    0     0     0 16137 17448 71  5 25  0  0
 9  0      0 336452 1069680 880620    0    0     0     0 15752 18798 70  7 23  0  0
 1  0      0 337784 1069680 880648    0    0     0     0 29450 27757 57  7 35  0  1
 3  0      0 337472 1069680 880740    0    0     0    40 31182 29661 56 13 32  0  0
^C
```

从上面的 r 列可以发现可运行的 task 数是多于 cpu core 的，也就是说会存在调度延迟，然后我们进一步观察 latency 情况。

```
root@ip-172-1-47-54:~# /usr/share/bcc/tools/runqlat 5 3 -p 15919
Tracing run queue latency... Hit Ctrl-C to end.

     usecs               : count     distribution
         0 -> 1          : 209      |                                        |
         2 -> 3          : 1146     |*****                                   |
         4 -> 7          : 2221     |*********                               |
         8 -> 15         : 9030     |****************************************|
        16 -> 31         : 2331     |**********                              |
        32 -> 63         : 800      |***                                     |
        64 -> 127        : 631      |**                                      |
       128 -> 255        : 762      |***                                     |
       256 -> 511        : 652      |**                                      |
       512 -> 1023       : 587      |**                                      |
      1024 -> 2047       : 347      |*                                       |
      2048 -> 4095       : 281      |*                                       |
      4096 -> 8191       : 50       |                                        |
      8192 -> 16383      : 13       |                                        |

     usecs               : count     distribution
         0 -> 1          : 101      |                                        |
         2 -> 3          : 719      |****                                    |
         4 -> 7          : 1616     |*********                               |
         8 -> 15         : 7073     |****************************************|
        16 -> 31         : 1873     |**********                              |
        32 -> 63         : 680      |***                                     |
        64 -> 127        : 478      |**                                      |
       128 -> 255        : 576      |***                                     |
       256 -> 511        : 499      |**                                      |
       512 -> 1023       : 367      |**                                      |
      1024 -> 2047       : 252      |*                                       |
      2048 -> 4095       : 150      |                                        |
      4096 -> 8191       : 30       |                                        |
      8192 -> 16383      : 1        |                                        |

     usecs               : count     distribution
         0 -> 1          : 80       |                                        |
         2 -> 3          : 656      |***                                     |
         4 -> 7          : 1387     |******                                  |
         8 -> 15         : 8028     |****************************************|
        16 -> 31         : 1761     |********                                |
        32 -> 63         : 531      |**                                      |
        64 -> 127        : 354      |*                                       |
       128 -> 255        : 413      |**                                      |
       256 -> 511        : 343      |*                                       |
       512 -> 1023       : 228      |*                                       |
      1024 -> 2047       : 154      |                                        |
      2048 -> 4095       : 84       |                                        |
      4096 -> 8191       : 11       |                                        |
      8192 -> 16383      : 1        |                                        |
```

会发现针对这个业务进程相关的延迟有时达到 1~2 ms 级别。所以我们可以理解为是**因为调度延迟问题导致的业务响应时间过高**。

我们在这台机器上运行了两个不同的 docker container ，每个 container 运行了 30 个 ruby threads 进行业务操作。cpu 利用率在高峰时可以达到 200% 多。

所以我们想进一步确认上面的信息是否完全证明是因为调度器的原因导致的业务延迟，以及相关的解决方案（分开部署？调整核心业务进程优先级？还是？）

Instance ID(s): i-048d059f387d6aa41


### 0x02 分析

感谢您提供了详细的分析说明， 从业务的设计角度来说，如果业务对时间非常敏感，将业务分开部署是一个好的方案。

您能否提供一下 `/usr/share/bcc/tools/runqlat` 工具的下载地址吗？我搜索了一下，有很多类似的工具。

同时如果方便，您留个电话，我们进一步沟通一下。

最后，对于这个问题我们这边还需要分析验证一下，还需要您的协助配合。谢谢。


### 0x03 信息补充

https://github.com/iovisor/bcc/blob/master/tools/runqlat.py


### 0x04 分析

分析过程：

首先感谢您提供 runqlat 的下载地址，我们也进行了详细的本地测试。

测试数据如下：

- 首先启动一台 c4.xlarge（4core) 的 ec2 实例，然后安装 runqlat 。
- 启动 8 个 cpu 消耗型的 stress 进程，通过 top 输出的最后一列观察到每个 core 分配了两个 stress 进程。同时观察到进程调度等待时间大部分都在 1ms 这个区间。

```
# stress -c 8 &
[1] 13732
stress: info: [13732] dispatching hogs: 8 cpu, 0 io, 0 vm, 0 hdd
```

```
# ./runqlat.py 3 1 -p 13733 -m
Tracing run queue latency... Hit Ctrl-C to end.

     msecs               : count     distribution
         0 -> 1          : 1502     |****************************************|

top:
 PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND            PPID P
13733 root      20   0    7324    100      0 R  49.9  0.0   2:10.03 stress            13732 1
13734 root      20   0    7324    100      0 R  49.9  0.0   2:10.16 stress            13732 1
13735 root      20   0    7324    100      0 R  49.9  0.0   2:09.99 stress            13732 2
13737 root      20   0    7324    100      0 R  49.9  0.0   2:10.03 stress            13732 2
13738 root      20   0    7324    100      0 R  49.9  0.0   2:09.97 stress            13732 0
13739 root      20   0    7324    100      0 R  49.9  0.0   2:10.32 stress            13732 3
13740 root      20   0    7324    100      0 R  49.9  0.0   2:10.00 stress            13732 3
13736 root      20   0    7324    100      0 R  49.6  0.0   2:10.06 stress            13732 0
```

- 启动 16 个 cpu 消耗型的 stress 进程，通过 top 输出的最后一列观察到每个 core 分配了四个 stress 进程。同时这个时候观察的进程调度等待时间大部分都在 3ms 这个区间。

```
# stress -c 16 &
[1] 13807
stress: info: [13807] dispatching hogs: 16 cpu, 0 io, 0 vm, 0 hdd
```

```
# ./runqlat.py 10 1 -p 13808 -m
Tracing run queue latency... Hit Ctrl-C to end.

     msecs               : count     distribution
         0 -> 1          : 5        |                                        |
         2 -> 3          : 2406     |****************************************|
         4 -> 7          : 93       |*                                       |
top:
  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND            PPID P
13808 root      20   0    7324    100      0 R  24.9  0.0   0:18.90 stress            13807 1
13809 root      20   0    7324    100      0 R  24.9  0.0   0:19.04 stress            13807 3
13810 root      20   0    7324    100      0 R  24.9  0.0   0:18.93 stress            13807 3
13811 root      20   0    7324    100      0 R  24.9  0.0   0:18.33 stress            13807 2
13812 root      20   0    7324    100      0 R  24.9  0.0   0:18.29 stress            13807 2
13813 root      20   0    7324    100      0 R  24.9  0.0   0:18.69 stress            13807 1
13814 root      20   0    7324    100      0 R  24.9  0.0   0:18.34 stress            13807 2
13815 root      20   0    7324    100      0 R  24.9  0.0   0:19.02 stress            13807 3
13816 root      20   0    7324    100      0 R  24.9  0.0   0:18.95 stress            13807 1
13817 root      20   0    7324    100      0 R  24.9  0.0   0:18.40 stress            13807 0
13818 root      20   0    7324    100      0 R  24.9  0.0   0:18.75 stress            13807 3
13819 root      20   0    7324    100      0 R  24.9  0.0   0:18.30 stress            13807 0
13820 root      20   0    7324    100      0 R  24.9  0.0   0:18.34 stress            13807 2
13821 root      20   0    7324    100      0 R  24.9  0.0   0:18.46 stress            13807 0
13822 root      20   0    7324    100      0 R  24.9  0.0   0:19.13 stress            13807 1
13823 root      20   0    7324    100      0 R  24.9  0.0   0:18.46 stress            13807 0
```

- 启动 32 个 cpu 消耗型的 stress 进程，通过 top 输出的最后一列观察到每个 core 分配了八个 stress 进程。同时这个时候观察的进程调度等待时间大部分都在 7ms 这个区间。

```
# stress -c 32 &
[1] 13830
# stress: info: [13830] dispatching hogs: 32 cpu, 0 io, 0 vm, 0 hdd
```

```
# ./runqlat.py 10 1 -p 13831 -m
Tracing run queue latency... Hit Ctrl-C to end.

     msecs               : count     distribution
         0 -> 1          : 3        |                                        |
         2 -> 3          : 0        |                                        |
         4 -> 7          : 1200     |****************************************|
         8 -> 15         : 50       |*                                       |
```

- 同时，根据 runqlat 的定义，其是用来检测当一个进程在运行队列上的时候，需要等待多久可以轮到（或者被调度到）cpu 上运行。但是当系统负载比较大的时候，这个进程需要等待更多的时间。

ref: http://manpages.ubuntu.com/manpages/zesty/man8/runqlat-bpfcc.8.html

```
This  measures  the  time  a  task  spends  waiting  on a run queue (or
       equivalent scheduler data structure) for a turn on-CPU, and shows  this
       time  as a histogram. This time should be small, but a task may need to
       wait its turn due to CPU load. The higher the CPU load,  the  longer  a
       task will generally need to wait its turn.
```

- 如上的测试也验证了这一点，同时也可以看到一个有趣的现象，就是每个 core 上运行的进程数量等于 runqlat 的检测输出值加一。
    - 也是就是当每个 core 运行进程为 2 的时候，需要等待 1ms 进行调度，因为进程 1 要等待进程 2 执行完成自己的时间片之后，才能被调度运行。这个时候时间片应该是 1ms 。 
    - 当每个 core 运行进程为 4 的时候，需要等待 3ms 进行调度，因为进程 1 要等待进程 2、进程 3、进程 4 执行完成自己的时间片之后，才能被调度运行。这个时候时间片应该是 1ms 。 
    - 当每个 core 运行进程为 8 的时候，需要等待 7ms 进行调度，因为进程 1 要等待进程 2，进程 3，进程 4，进程 5，进程 6，进程 7，进程 8 执行完成自己的时间片之后，才能被调度运行。这个时候时间片应该是 1ms 。

解决方案：

综上，问题产生的原因还是由于系统负载过高导致的，每个进程需要等待更多的时间才能被分配到 CPU 资源从而运行，所以您将业务分开部署还是非常符合最佳时间的，从而也帮助您的业务平稳运行。













